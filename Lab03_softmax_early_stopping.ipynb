{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yêu cầu: Implement Batch Gradient Descent with early stopping for Softmax Regression without using Scikit-Learn (trừ train_test_split)\n",
    "- Data: iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']# coi thứ tự "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: có 4 cột + 1 cột class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chia tập dữ liệu thành 3 tập train/test/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị X_df và y_sr\n",
    "- Trong lab này chỉ lấy 2 thuộc tính: petal length, petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thêm cột bias (i.e. $x_0$ = 1) cho X vào lưu vào `X_b` (X with bias). Lưu ý về mặt kí hiệu của x: lowerscript = cột thứ mấy; superscript = hàng thứ mấy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.4, 0.2],\n",
       "       [1. , 1.4, 0.2],\n",
       "       [1. , 1.3, 0.2],\n",
       "       [1. , 1.5, 0.2],\n",
       "       [1. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.concatenate((np.ones((len(X), 1)), X), axis=1)\n",
    "X_b[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chia tập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 24 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 1, 0, 0, 1, 2, 0, 0, 2,\n",
       "       2, 1, 2, 1, 1, 0, 2, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 0, 0,\n",
       "       0, 2, 0, 0, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 2, 0,\n",
       "       0, 0, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2,\n",
       "       1, 1, 0, 2, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_b, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train), len(X_val), len(X_test))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: `y_train` đã mã hóa sẵn 3 loài hoa thành 3 số khác nhau. Tuy nhiên để xài dc softmax thì `y_train` phải là matrix mx3 (m = len(X_train)) là để xíu xài cho gradient có đụng. Xét 1 hàng/instance x, với mỗi cột k (trong 3 cột) là xác suất x thuộc class k đó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tạo 1 encoder và dùng public method `toarray()` của lớp này để mã hóa (fit_transform) `y_train` trước theo ý tưởng trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y_train_new = enc.fit_transform(y_train.reshape(-1,1)) # fit_transform() expect vector cột nên phải reshape\n",
    "y_train_new = y_train_new.toarray() # convert to np array để dùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "print(y_train[:5])\n",
    "y_train_new[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Giờ encode trên `y_val` và `y_test` (chỉ gọi `transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_new = enc.transform(y_val.reshape(-1,1)).toarray()\n",
    "y_test_new = enc.transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Với 1 instance x thì $s_k(x)$ gọi là logit, có kích thước 1x3 lưu score cho mỗi class mà x thuộc về \n",
    "- Tuy nhiên, cái ta tính là tính trên cả dataset (do dùng numpy) nên logit**s** là array của các logit đơn -> logit**s** có kích thước mx3\n",
    "    - 1 con x (1x3, 3=2+1), 1 class k (3x1): $s_k(x) = x^T \\theta$ = 1 số\n",
    "    - 1 con x (1x3), k=3 class (3x3): `logit` = $s(x) = x^T \\Theta = \\begin{bmatrix} s_0(x) & s_1(x) & s_2(x) \\end{bmatrix}$ = matrix kích thước 1x3 -> score cho mỗi class của 1 con x\n",
    "    - m con x (mx3), k=3 class (3x3): `logits` = $\\begin{bmatrix} s_0(x_0) & s_1(x_0) & s_2(x_0) \\\\ s_0(x_1) & s_1(x_1) & s_2(x_1) \\\\ ... \\end{bmatrix}$ = matrix kích thước mx3 -> mỗi hàng là mỗi con $s(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_hat\n",
    "def softmax(logits):\n",
    "    '''logits: shape = mxk, mỗi hàng thể hiện xác suất của 1 instance thuộc vào k class'''\n",
    "    nume = np.exp(logits) # m x 3\n",
    "    deno = np.sum(nume, axis=1, keepdims=True) # cộng theo hàng -> m x 1\n",
    "    return nume/deno # m x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khởi tạo Theta $\\Theta$ (theta lớn) có shape = (n+1, K) với mỗi cột là 1 bộ tham số $\\theta _i$ của mỗi class; ứng với mỗi cột thì từng hàng là feature_weight với $w_0$ = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
    "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)\n",
    "\n",
    "def init_Theta():\n",
    "    Theta = np.random.randn(n_inputs, n_outputs)\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sử dụng code của thuật toán Batch GD để training tìm ra bộ tham số mô hình $\\Theta$ tốt nhất. Trong quá đó lồng ghép 2 CT chính sau đây để giải:\n",
    "1. The cost function: $J(\\Theta) = - \\frac 1 m \\sum \\limits _{i=1} ^{m} \\sum \\limits _{k=1} ^{K} y_k^{(i)} log(\\hat{p}_k^{(i)})$ = $mean(sum(y_k^{(i)} log(\\hat{p}_k^{(i)}), axis=1))$ = mean(sum(`y_train_new`*log(`y_proba`))) \n",
    "    - trong numpy, để tránh log(0) = nan, ta cộng 1 lượng `eps` vào log\n",
    "2. gradient - đạo hàm từng phần: $\\nabla _{\\Theta}J(\\Theta) = \\frac 1 m XE^T$ với $E = \\hat{P} - y\\_train\\_new$\n",
    "- Thấy rằng cả 2 CT trên đều dùng $\\hat{p}_k^{(i)}$ = `y_proba` là kq của softmax function nên tính $\\hat{p}_k^{(i)}$ trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.671746502962418\n",
      "500 0.513541064240001\n",
      "1000 0.4008764512320962\n",
      "1500 0.34751149803877507\n",
      "2000 0.31369883147140937\n",
      "2500 0.2893722811067073\n",
      "3000 0.2706102255796254\n",
      "3500 0.25550070017188903\n",
      "4000 0.24296973909342823\n",
      "4500 0.232352702981127\n",
      "5000 0.22320874349690198\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "eps = 1e-7\n",
    "\n",
    "Theta = init_Theta()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # tính P_hat = y_proba\n",
    "    logits = X_train.dot(Theta) # shape = (m,3)\n",
    "    y_proba = softmax(logits) # shape = (m,3)\n",
    "    \n",
    "    # tính loss function và in ra sau mỗi 500 vòng lặp, ta kỳ vọng loss luôn giảm sau mỗi iter\n",
    "    if i % 500 == 0:\n",
    "        loss = -np.mean(np.sum(y_train_new * np.log(y_proba + eps), axis=1)) # tích có shape = (m,3)\n",
    "        print(i, loss)\n",
    "\n",
    "    # tính hiệu trong hàm gradient: error\n",
    "    E = y_proba - y_train_new # (m,3) - (m,1) = (m,3)\n",
    "    gradient = 1/m * X_train.T.dot(E) # (n+1,3) = (3,3)\n",
    "    Theta = Theta - learning_rate*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Giải thích shape của 1 số phép toán**:\n",
    "- `logits`.shape = (m,3): mỗi hàng (i.e. 1 instance) chứa 3 score cho 3 lớp -> broadcast cho m điểm dữ liệu tương ứng m hàng (lưu ý score có thể âm or dương)\n",
    "- `y_proba`.shape = `logits`.shape = (m,3): mỗi hàng (i.e. 1 instance) chứa 3 probability cho 3 lớp -> broadcast cho m điểm dữ liệu tương ứng m hàng\n",
    "- `gradient`.shape = (n+1,3) = (3,3): mỗi hàng là mỗi trọng số ($w_0$ - bias, $w_1$, $w_2$), mỗi cột là mỗi theta (i.e. mỗi class). Lý do có shape này đó là, để đạo hàm trên hàm TQ $J(\\Theta)$, lấy đạo hàm từng phần trên mỗi $\\theta _i$; mà mỗi $\\theta _i$ lại có n+1=2+1=3 trọng số, như vậy lại phải lấy đạo hàm từng phần của $\\theta _i$ trên 3 trọng số. Kết quả là `gradient` \n",
    "    - Nhớ lại lý thuyết, mỗi vector cột trong `gradient` là vector luôn đi lên dốc (uphill direction) nên lấy từng cột $\\theta$ trong $\\Theta$ trừ đi `learning_rate`*từng cột trong `gradient` sẽ update dc $\\Theta$ (coi lại Batch GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.86782215,  0.54140246, -6.40308622],\n",
       "       [-1.09227558,  1.0119759 ,  1.13503921],\n",
       "       [-1.5704224 , -0.2863346 ,  3.73393016]])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Giờ ta dự đoán trên tập valid để đánh giá performace. Tập valid có m' dòng, 3 cột"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_val.dot(Theta) # shape = (m',3) \n",
    "y_proba = softmax(logits) # shape = (m',3)\n",
    "y_val_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "# accuracy\n",
    "val_acc = np.mean(y_val_pred == y_val)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: bằng việc toggle biến `n_iterations` và `learning_rate` thì hên nó ra cao, VD: val_acc = 91.66%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model có sử dụng regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong các hàm của scikit-learn thì default bên dưới có sử dụng regularization nên ta sẽ lập trình lại phần này luôn và ta chọn *Ridge regularization* (tức regularization term = l2)\n",
    "- Như vậy từ đoạn code cũ cần chỉnh:\n",
    "    - Định nghĩa lại hàm loss (i.e. đề bài): $J(\\Theta) = J(\\Theta) + \\alpha * regularization\\_term$ với `regularization_term` = $\\alpha * \\frac 1 2 \\sum \\limits _{i=1} ^n w_i^2 = l_2$\n",
    "    - Do sử dụng Batch GD nên `gradient` = `gradient` + $\\alpha$.**w**, vơi **w** = [$w_1$, $w_2$, ..., $w_n$]$^T$ (ko regularize $w_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.351186178750213\n",
      "500 0.5363074274879401\n",
      "1000 0.5103361469887799\n",
      "1500 0.5028300175534949\n",
      "2000 0.5000930424932675\n",
      "2500 0.49900726252113436\n",
      "3000 0.49855778929510985\n",
      "3500 0.4983671456117274\n",
      "4000 0.49828508447873926\n",
      "4500 0.49824943422167567\n",
      "5000 0.4982338546809579\n",
      "5500 0.498227020110083\n",
      "6000 0.4982240143362766\n",
      "6500 0.4982226902490736\n",
      "7000 0.4982221063362394\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1 # tăng\n",
    "n_iterations = 7001\n",
    "m = len(X_train)\n",
    "eps = 1e-7\n",
    "alpha = 0.1 # hyperparameter\n",
    "\n",
    "Theta = init_Theta()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # tính P_hat = y_proba\n",
    "    logits = X_train.dot(Theta) # shape = (m,3)\n",
    "    y_proba = softmax(logits) # shape = (m,3)\n",
    "    \n",
    "    # tính loss function và in ra sau mỗi 500 vòng lặp, ta kỳ vọng loss luôn giảm sau mỗi iter\n",
    "    if i % 500 == 0:\n",
    "        loss = -np.mean(np.sum(y_train_new * np.log(y_proba + eps), axis=1)) # tích có shape = (m,3)\n",
    "        l2 = 1/2 * np.sum(np.square(Theta[1:])) # sum theo từng theta (hay axis=0) trong Theta \n",
    "        loss = loss + alpha * l2\n",
    "        print(i, loss)\n",
    "\n",
    "    # tính hiệu trong hàm gradient: error\n",
    "    E = y_proba - y_train_new # (m,3) - (m,1) = (m,3)\n",
    "    gradient = 1/m * X_train.T.dot(E) + alpha * np.r_[np.zeros([1, n_outputs]), Theta[1:]] # (n+1, 3)\n",
    "    Theta = Theta - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Giải thích shape của 1 số phép toán**:\n",
    "- `gradient`.shape = (n+1,3), shape ko đổi so với code cũ:  \n",
    "    - `1/m * X_train.T.dot(E)` -> shape = (n+1,3): chính là gradient cũ, coi lại gthich ở các cell trên\n",
    "    - `np.r_[np.zeros([1, n_outputs]), Theta[1:]]` = **w** = vector cột -> shape = (n+1,3): do ta ko regularize $w_0$ nên hàng đầu của  **w** chứa số 0 (tức `np.zeros([1, n_outputs])`), còn các hàng còn lại chứ [$w_1$, $w_2$, ..., $w_n$]$^T$ (tức `Theta[1:]`)\n",
    "    - xong lấy tổng thì shape vẫn là (n+1,3), có điều hàng $0^{th}$ ko thay đổi sau khi cộng (thỏa mãn \"ko regularize $w_0$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_val.dot(Theta)\n",
    "y_proba = softmax(logits)\n",
    "y_val_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "# accuracy\n",
    "np.mean(y_val_pred == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training có sử dụng regularization + early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ta chỉ cập nhật Theta trên X_train, X_val dc thêm vào chỉ mang t/c lợi dùng MSE_val của nó để làm DK dừng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.584472127756714\n",
      "500 0.6083720338049515\n",
      "1000 0.5618544074845901\n",
      "1500 0.5472862318780056\n",
      "2000 0.5411347650851447\n",
      "2500 0.5381617580416181\n",
      "3000 0.5365935456005979\n",
      "3500 0.5357118359742908\n",
      "4000 0.5351918062819365\n",
      "4500 0.5348739945642981\n",
      "5000 0.5346746804486506\n",
      "5500 0.5345473625599602\n",
      "6000 0.5344649844756109\n",
      "6500 0.5344112113320107\n",
      "7000 0.5343758989861248\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1 # tăng\n",
    "n_iterations = 7001\n",
    "m = len(X_train)\n",
    "eps = 1e-7\n",
    "alpha = 0.1 # hyperparameter\n",
    "best_loss = np.inf\n",
    "\n",
    "Theta = init_Theta()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # tính trên X_train\n",
    "    logits = X_train.dot(Theta) # shape = (m,3)\n",
    "    y_proba = softmax(logits) # shape = (m,3)\n",
    "    E = y_proba - y_train_new # (m,3) - (m,1) = (m,3)\n",
    "    gradient = 1/m * X_train.T.dot(E) + alpha * np.r_[np.zeros([1, n_outputs]), Theta[1:]] # (n+1, 3)\n",
    "    Theta = Theta - learning_rate * gradient\n",
    "    \n",
    "    # tính trên X_val\n",
    "    logits = X_val.dot(Theta) # shape = (m,3)\n",
    "    y_proba = softmax(logits) # shape = (m,3)\n",
    "    \n",
    "    ## tính MSE_val\n",
    "    loss = -np.mean(np.sum(y_val_new * np.log(y_proba + eps), axis=1)) # tích có shape = (m,3)\n",
    "    l2 = 1/2 * np.sum(np.square(Theta[1:])) # sum theo từng theta (hay axis=0) trong Theta \n",
    "    loss = loss + alpha * l2\n",
    "    if i % 500 == 0:\n",
    "        print(i, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else: # DK dừng: ngay khi MSE_val tăng thì dừng gấp\n",
    "        print(i - 1, best_loss) # show best_loss\n",
    "        print(i, loss, \"early stopping!\") # show loss ngay khi lớn best_loss \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_val.dot(Theta)\n",
    "y_proba = softmax(logits)\n",
    "y_val_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "# accuracy\n",
    "np.mean(y_val_pred == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: tập validation có dùng early stopping dự đoán đúng 91.67%. Bây giờ dự đoán trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test.dot(Theta)\n",
    "y_proba = softmax(logits)\n",
    "y_test_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_test_pred == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: dự đoán đúng 100%, có lẽ do tập test nhỏ mới đạt dc độ hoàn hảo như vầy.."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
